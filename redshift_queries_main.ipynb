{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66fa703e",
   "metadata": {},
   "source": [
    "# Amazon Redshift (Provisioned)\n",
    "\n",
    "\n",
    "### Cluster Configuration\n",
    "- **Cluster**: wk08-redshift-cluster\n",
    "- **Database**: dev\n",
    "- **User**: rsadmin\n",
    "- **Region**: us-west-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e996c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "session = boto3.Session(region_name='us-west-2', profile_name='default')\n",
    "client = session.client('redshift-data')\n",
    "\n",
    "CLUSTER_ID = 'wk08-redshift-cluster'\n",
    "DATABASE = 'dev'\n",
    "DB_USER = 'rsadmin'\n",
    "REGION = 'us-west-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "115c9b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_and_wait(sql, cluster_id, database, db_user):\n",
    "    response = client.execute_statement(\n",
    "        ClusterIdentifier=cluster_id,\n",
    "        Database=database,\n",
    "        DbUser=db_user,\n",
    "        Sql=sql\n",
    "    )\n",
    "    stmt_id = response['Id']\n",
    "    print(f\"Statement ID: {stmt_id}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        status = client.describe_statement(Id=stmt_id)\n",
    "        if status['Status'] in ['FINISHED', 'FAILED', 'ABORTED']:\n",
    "            break\n",
    "        time.sleep(2) \n",
    "        \n",
    "    end_time = time.time()\n",
    "    wall_clock_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Status: {status['Status']}\")\n",
    "    if status['Status'] == 'FINISHED':\n",
    "        duration_ns = status.get('Duration', 0)\n",
    "        duration_sec = duration_ns / 1_000_000_000 if duration_ns > 0 else 0\n",
    "        print(f\"Execution time: {duration_sec:.2f} seconds\")\n",
    "        print(f\"Wall clock time: {wall_clock_time:.2f} seconds\")\n",
    "        \n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b675e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement ID: 399a1dd5-b870-40b5-acd5-52517337b15f\n",
      "Status: FINISHED\n",
      "Execution time: 0.03 seconds\n",
      "Wall clock time: 2.14 seconds\n",
      "Status: FINISHED\n",
      "Execution time: 0.03 seconds\n",
      "Wall clock time: 2.14 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example: Run EXPLAIN and capture plan\n",
    "query_sql = \"\"\"\n",
    "SELECT 'optimized_orders' as table_name, COUNT(*) as row_count\n",
    "FROM optimized_orders;\n",
    "\"\"\"\n",
    "\n",
    "explain_result = execute_and_wait(\n",
    "    f\"EXPLAIN {query_sql}\",\n",
    "    # grab the below values from your stack output\n",
    "    cluster_id='wk08-redshift-cluster',\n",
    "    database='dev',\n",
    "    db_user='rsadmin'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f7c5107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_results(statement_id, max_rows=100):\n",
    "    response = client.get_statement_result(\n",
    "        Id=statement_id,\n",
    "        NextToken=''\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c33ba341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ColumnMetadata': [{'isCaseSensitive': True,\n",
       "   'isCurrency': False,\n",
       "   'isSigned': False,\n",
       "   'label': 'QUERY PLAN',\n",
       "   'length': 0,\n",
       "   'name': 'QUERY PLAN',\n",
       "   'nullable': 1,\n",
       "   'precision': 65535,\n",
       "   'scale': 0,\n",
       "   'schemaName': '',\n",
       "   'tableName': '',\n",
       "   'typeName': 'varchar'}],\n",
       " 'Records': [[{'stringValue': 'XN Aggregate  (cost=375000.00..375000.00 rows=1 width=0)'}],\n",
       "  [{'stringValue': '  ->  XN Seq Scan on optimized_orders  (cost=0.00..300000.00 rows=30000000 width=0)'}]],\n",
       " 'TotalNumRows': 2,\n",
       " 'ResponseMetadata': {'RequestId': '46d66250-6fa3-461e-b13d-8a3ac52eac23',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '46d66250-6fa3-461e-b13d-8a3ac52eac23',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '436',\n",
       "   'date': 'Fri, 21 Nov 2025 03:05:52 GMT',\n",
       "   'connection': 'close'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_id='399a1dd5-b870-40b5-acd5-52517337b15f'\n",
    "get_query_results(s_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6a6918",
   "metadata": {},
   "source": [
    "## Query Definitions\n",
    "\n",
    "We'll execute two analytical queries against both the Spectrum external table and the native Redshift table to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Aggregation with selective time filter\n",
    "query1_template = \"\"\"\n",
    "SELECT\n",
    "    category,\n",
    "    region,\n",
    "    COUNT(*) as order_count,\n",
    "    SUM(extended_price) as total_revenue,\n",
    "    AVG(discount_rate) as avg_discount,\n",
    "    COUNT(DISTINCT product_sku) as unique_products\n",
    "FROM {table_name}\n",
    "WHERE ts >= '2024-06-01' AND ts < '2024-07-01'\n",
    "    AND category IN ('analytics', 'compute', 'observability')\n",
    "    AND region IN ('us-east', 'us-west', 'eu-west')\n",
    "GROUP BY category, region\n",
    "ORDER BY total_revenue DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Query 2: Multi-range temporal scan\n",
    "query2_template = \"\"\"\n",
    "SELECT\n",
    "    category,\n",
    "    status,\n",
    "    COUNT(*) as order_count,\n",
    "    SUM(extended_price) as total_revenue,\n",
    "    AVG(quantity) as avg_quantity,\n",
    "    MIN(ts) as earliest_order,\n",
    "    MAX(ts) as latest_order\n",
    "FROM {table_name}\n",
    "WHERE (\n",
    "    (ts >= '2023-03-01' AND ts < '2023-03-15')\n",
    "    OR (ts >= '2023-09-01' AND ts < '2023-09-15')\n",
    "    OR (ts >= '2024-03-01' AND ts < '2024-03-15')\n",
    ")\n",
    "    AND category = 'analytics'\n",
    "GROUP BY category, status\n",
    "ORDER BY total_revenue DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Tables\n",
    "spectrum_table = \"spectrum_raw.orders\"\n",
    "native_table = \"optimized_orders\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b38af7",
   "metadata": {},
   "source": [
    "## Performance Analysis Results\n",
    "\n",
    "### Query 1: Aggregation with Selective Time Filter\n",
    "\n",
    "This query tests performance on time-based filtering with aggregations across multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a42fc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Query 1 Analysis - Spectrum Table\n",
      "Statement ID: 4deab60c-80d2-4411-a37b-d6fa45d76975\n",
      "Status: FINISHED\n",
      "Execution time: 0.30 seconds\n",
      "Wall clock time: 2.15 seconds\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store results\n",
    "results = {\n",
    "    'query1': {'spectrum': {}, 'native': {}},\n",
    "    'query2': {'spectrum': {}, 'native': {}}\n",
    "}\n",
    "\n",
    "print(\"Starting Query 1 Analysis - Spectrum Table\")\n",
    "\n",
    "# Query 1 - Spectrum EXPLAIN\n",
    "query1_spectrum = query1_template.format(table_name=spectrum_table)\n",
    "explain1_spectrum = execute_and_wait(\n",
    "    f\"EXPLAIN {query1_spectrum}\",\n",
    "    CLUSTER_ID, DATABASE, DB_USER\n",
    ")\n",
    "results['query1']['spectrum']['explain'] = explain1_spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99cfa8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement ID: ff9df78f-be2a-4542-8c52-7ec21a6cc42d\n",
      "Status: FINISHED\n",
      "Execution time: 65.44 seconds\n",
      "Wall clock time: 66.39 seconds\n",
      "\n",
      "Query 1 Spectrum completed in 65.44 seconds\n",
      "Status: FINISHED\n",
      "Execution time: 65.44 seconds\n",
      "Wall clock time: 66.39 seconds\n",
      "\n",
      "Query 1 Spectrum completed in 65.44 seconds\n"
     ]
    }
   ],
   "source": [
    "# Query 1 - Spectrum execution\n",
    "query1_spectrum_result = execute_and_wait(\n",
    "    query1_spectrum,\n",
    "    CLUSTER_ID, DATABASE, DB_USER\n",
    ")\n",
    "results['query1']['spectrum']['execution'] = query1_spectrum_result\n",
    "\n",
    "if query1_spectrum_result['Status'] == 'FINISHED':\n",
    "    spectrum_q1_duration = query1_spectrum_result.get('Duration', 0) / 1_000_000_000\n",
    "    print(f\"\\nQuery 1 Spectrum completed in {spectrum_q1_duration:.2f} seconds\")\n",
    "else:\n",
    "    print(f\"\\nQuery 1 Spectrum failed: {query1_spectrum_result.get('Error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cd62ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Query 1 Analysis - Native Table\n",
      "Statement ID: 104f9e27-e36e-4171-ae44-7e2278639841\n",
      "Statement ID: 104f9e27-e36e-4171-ae44-7e2278639841\n",
      "Status: FINISHED\n",
      "Execution time: 0.04 seconds\n",
      "Wall clock time: 2.13 seconds\n",
      "Status: FINISHED\n",
      "Execution time: 0.04 seconds\n",
      "Wall clock time: 2.13 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Query 1 Analysis - Native Table\")\n",
    "\n",
    "# Query 1 - Native EXPLAIN\n",
    "query1_native = query1_template.format(table_name=native_table)\n",
    "explain1_native = execute_and_wait(\n",
    "    f\"EXPLAIN {query1_native}\",\n",
    "    CLUSTER_ID, DATABASE, DB_USER\n",
    ")\n",
    "results['query1']['native']['explain'] = explain1_native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a1cf65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement ID: 244ad6b0-f155-4d7c-97c0-c3691b1b65c1\n",
      "Status: FINISHED\n",
      "Execution time: 5.26 seconds\n",
      "Wall clock time: 6.31 seconds\n",
      "\n",
      "Query 1 Native completed in 5.26 seconds\n",
      "Status: FINISHED\n",
      "Execution time: 5.26 seconds\n",
      "Wall clock time: 6.31 seconds\n",
      "\n",
      "Query 1 Native completed in 5.26 seconds\n"
     ]
    }
   ],
   "source": [
    "# Query 1 - Native execution\n",
    "query1_native_result = execute_and_wait(\n",
    "    query1_native,\n",
    "    CLUSTER_ID, DATABASE, DB_USER\n",
    ")\n",
    "results['query1']['native']['execution'] = query1_native_result\n",
    "\n",
    "if query1_native_result['Status'] == 'FINISHED':\n",
    "    native_q1_duration = query1_native_result.get('Duration', 0) / 1_000_000_000\n",
    "    print(f\"\\nQuery 1 Native completed in {native_q1_duration:.2f} seconds\")\n",
    "else:\n",
    "    print(f\"\\nQuery 1 Native failed: {query1_native_result.get('Error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ebb06",
   "metadata": {},
   "source": [
    "### Query 2: Multi-range Temporal Scan\n",
    "\n",
    "This query tests performance on complex temporal filtering with multiple date ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d4c1b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Query 2 Analysis - Spectrum Table\n",
      "Statement ID: e061e920-f25d-4a1a-bc11-aa3afa001106\n",
      "Statement ID: e061e920-f25d-4a1a-bc11-aa3afa001106\n",
      "Status: FINISHED\n",
      "Execution time: 0.25 seconds\n",
      "Wall clock time: 2.13 seconds\n",
      "Status: FINISHED\n",
      "Execution time: 0.25 seconds\n",
      "Wall clock time: 2.13 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Query 2 Analysis - Spectrum Table\")\n",
    "\n",
    "# Query 2 - Spectrum EXPLAIN\n",
    "query2_spectrum = query2_template.format(table_name=spectrum_table)\n",
    "explain2_spectrum = execute_and_wait(\n",
    "    f\"EXPLAIN {query2_spectrum}\",\n",
    "    CLUSTER_ID, DATABASE, DB_USER\n",
    ")\n",
    "results['query2']['spectrum']['explain'] = explain2_spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcc3e23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement ID: 0f731041-d8b0-4c18-ae4f-d7403b4b66ec\n",
      "Status: FINISHED\n",
      "Execution time: 63.98 seconds\n",
      "Wall clock time: 64.45 seconds\n",
      "\n",
      "Query 2 Spectrum completed in 63.98 seconds\n",
      "Status: FINISHED\n",
      "Execution time: 63.98 seconds\n",
      "Wall clock time: 64.45 seconds\n",
      "\n",
      "Query 2 Spectrum completed in 63.98 seconds\n"
     ]
    }
   ],
   "source": [
    "# Query 2 - Spectrum execution\n",
    "query2_spectrum_result = execute_and_wait(\n",
    "    query2_spectrum,\n",
    "    CLUSTER_ID, DATABASE, DB_USER\n",
    ")\n",
    "results['query2']['spectrum']['execution'] = query2_spectrum_result\n",
    "\n",
    "if query2_spectrum_result['Status'] == 'FINISHED':\n",
    "    spectrum_q2_duration = query2_spectrum_result.get('Duration', 0) / 1_000_000_000\n",
    "    print(f\"\\nQuery 2 Spectrum completed in {spectrum_q2_duration:.2f} seconds\")\n",
    "else:\n",
    "    print(f\"\\nQuery 2 Spectrum failed: {query2_spectrum_result.get('Error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b62e5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Query 2 Analysis - Native Table\n",
      "Statement ID: a060ecb9-fae4-4fa7-959b-21a325c0241a\n",
      "Statement ID: a060ecb9-fae4-4fa7-959b-21a325c0241a\n",
      "Status: FINISHED\n",
      "Execution time: 0.04 seconds\n",
      "Wall clock time: 2.15 seconds\n",
      "Status: FINISHED\n",
      "Execution time: 0.04 seconds\n",
      "Wall clock time: 2.15 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Query 2 Analysis - Native Table\")\n",
    "\n",
    "# Query 2 - Native EXPLAIN\n",
    "query2_native = query2_template.format(table_name=native_table)\n",
    "explain2_native = execute_and_wait(\n",
    "    f\"EXPLAIN {query2_native}\",\n",
    "    CLUSTER_ID, DATABASE, DB_USER\n",
    ")\n",
    "results['query2']['native']['explain'] = explain2_native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2387dcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement ID: 4fd3d5ca-4a97-4179-8dc3-1d39b94f2a3c\n",
      "Status: FINISHED\n",
      "Execution time: 6.50 seconds\n",
      "Wall clock time: 8.37 seconds\n",
      "\n",
      "Query 2 Native completed in 6.50 seconds\n",
      "Status: FINISHED\n",
      "Execution time: 6.50 seconds\n",
      "Wall clock time: 8.37 seconds\n",
      "\n",
      "Query 2 Native completed in 6.50 seconds\n"
     ]
    }
   ],
   "source": [
    "# Query 2 - Native execution\n",
    "query2_native_result = execute_and_wait(\n",
    "    query2_native,\n",
    "    CLUSTER_ID, DATABASE, DB_USER\n",
    ")\n",
    "results['query2']['native']['execution'] = query2_native_result\n",
    "\n",
    "if query2_native_result['Status'] == 'FINISHED':\n",
    "    native_q2_duration = query2_native_result.get('Duration', 0) / 1_000_000_000\n",
    "    print(f\"\\nQuery 2 Native completed in {native_q2_duration:.2f} seconds\")\n",
    "else:\n",
    "    print(f\"\\nQuery 2 Native failed: {query2_native_result.get('Error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6da38d7",
   "metadata": {},
   "source": [
    "## Table Metadata and Statistics\n",
    "\n",
    "After running all queries, let's analyze the native table and get storage comparison metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "384ac89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement ID: cafc8b4f-7254-4211-b928-46a84ab96c49\n",
      "Status: FINISHED\n",
      "Execution time: 0.35 seconds\n",
      "Wall clock time: 2.14 seconds\n",
      "Status: FINISHED\n",
      "Execution time: 0.35 seconds\n",
      "Wall clock time: 2.14 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run ANALYZE on the native table\n",
    "analyze_result = execute_and_wait(\n",
    "    \"ANALYZE optimized_orders;\",\n",
    "    CLUSTER_ID, DATABASE, DB_USER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3e5e462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement ID: 7b2d49d0-55d1-4cc4-bb03-7dd2f4a424ef\n",
      "Status: FINISHED\n",
      "Execution time: 6.27 seconds\n",
      "Wall clock time: 8.45 seconds\n",
      "Table statistics retrieved successfully\n",
      "Status: FINISHED\n",
      "Execution time: 6.27 seconds\n",
      "Wall clock time: 8.45 seconds\n",
      "Table statistics retrieved successfully\n"
     ]
    }
   ],
   "source": [
    "# Get table metadata from svv_table_info\n",
    "table_stats_sql = \"\"\"\n",
    "SELECT\n",
    "    \"table\",\n",
    "    size as size_mb,\n",
    "    tbl_rows,\n",
    "    sortkey1,\n",
    "    diststyle,\n",
    "    unsorted as unsorted_pct,\n",
    "    encoded\n",
    "FROM svv_table_info\n",
    "WHERE \"table\" = 'optimized_orders';\n",
    "\"\"\"\n",
    "\n",
    "table_stats_result = execute_and_wait(\n",
    "    table_stats_sql,\n",
    "    CLUSTER_ID, DATABASE, DB_USER\n",
    ")\n",
    "\n",
    "# Get the results\n",
    "if table_stats_result['Status'] == 'FINISHED':\n",
    "    table_stats_data = get_query_results(table_stats_result['Id'])\n",
    "    print(\"Table statistics retrieved successfully\")\n",
    "else:\n",
    "    print(\"Failed to get table statistics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24bd25c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ClusterIdentifier': 'wk08-redshift-cluster',\n",
       " 'CreatedAt': datetime.datetime(2025, 11, 20, 19, 14, 12, 689000, tzinfo=tzlocal()),\n",
       " 'Database': 'dev',\n",
       " 'DbUser': 'rsadmin',\n",
       " 'Duration': 6269925890,\n",
       " 'HasResultSet': True,\n",
       " 'Id': '7b2d49d0-55d1-4cc4-bb03-7dd2f4a424ef',\n",
       " 'QueryString': '\\nSELECT\\n    \"table\",\\n    size as size_mb,\\n    tbl_rows,\\n    sortkey1,\\n    diststyle,\\n    unsorted as unsorted_pct,\\n    encoded\\nFROM svv_table_info\\nWHERE \"table\" = \\'optimized_orders\\';\\n',\n",
       " 'RedshiftPid': 1073955057,\n",
       " 'RedshiftQueryId': 3946,\n",
       " 'ResultFormat': 'json',\n",
       " 'ResultRows': 1,\n",
       " 'ResultSize': 75,\n",
       " 'Status': 'FINISHED',\n",
       " 'UpdatedAt': datetime.datetime(2025, 11, 20, 19, 14, 19, 449000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '3696b5e5-dd57-4c8b-9850-d40c4f1ade54',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '3696b5e5-dd57-4c8b-9850-d40c4f1ade54',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '561',\n",
       "   'date': 'Fri, 21 Nov 2025 03:14:21 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_stats_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30aa70e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement ID: c2113a5c-65cf-49be-b59f-bbe214a28d2b\n",
      "Status: FINISHED\n",
      "Execution time: 4.44 seconds\n",
      "Wall clock time: 6.38 seconds\n",
      "Status: FINISHED\n",
      "Execution time: 4.44 seconds\n",
      "Wall clock time: 6.38 seconds\n"
     ]
    }
   ],
   "source": [
    "# Get row counts for comparison\n",
    "row_count_sql = \"\"\"\n",
    "SELECT 'spectrum_raw.orders' as table_name, COUNT(*) as row_count\n",
    "FROM spectrum_raw.orders\n",
    "UNION ALL\n",
    "SELECT 'optimized_orders' as table_name, COUNT(*) as row_count\n",
    "FROM optimized_orders;\n",
    "\"\"\"\n",
    "\n",
    "row_count_result = execute_and_wait(\n",
    "    row_count_sql,\n",
    "    CLUSTER_ID, DATABASE, DB_USER\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959d9251",
   "metadata": {},
   "source": [
    "## Performance Analysis Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd61ed4",
   "metadata": {},
   "source": [
    "### 1. EXPLAIN Plan Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8eebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 1 - Spectrum Table EXPLAIN Plan:\n",
      "============================================================\n",
      "XN Merge  (cost=1000300000345.97..1000300000348.75 rows=1114 width=295)\n",
      "  Merge Key: sum(extended_price)\n",
      "  ->  XN Network  (cost=1000300000345.97..1000300000348.75 rows=1114 width=295)\n",
      "        Send to leader\n",
      "        ->  XN Sort  (cost=1000300000345.97..1000300000348.75 rows=1114 width=295)\n",
      "              Sort Key: sum(extended_price)\n",
      "              ->  XN HashAggregate  (cost=300000278.45..300000289.59 rows=1114 width=295)\n",
      "                    ->  XN S3 Query Scan orders  (cost=0.00..300000111.38 rows=11138 width=295)\n",
      "                          ->  S3 Seq Scan spectrum_raw.orders location:\"s3://csed516-shared-resources-au2025-sharedclassbucket-mc0zal7gjcmu/class08/raw\" format:ION  (cost=0.00..300000000.00 rows=11138 width=295)\n",
      "                                Filter: ((\"ts\" < '2024-07-01 00:00:00'::timestamp without time zone) AND (\"ts\" >= '2024-06-01 00:00:00'::timestamp without time zone) AND (((category)::text = 'analytics'::text) OR ((category)::text = 'compute'::text) OR ((category)::text = 'observability'::text)) AND (((\"region\")::text = 'eu-west'::text) OR ((\"region\")::text = 'us-east'::text) OR ((\"region\")::text = 'us-west'::text)))\n",
      "\n",
      "Query 1 - Native Table EXPLAIN Plan:\n",
      "============================================================\n",
      "XN Merge  (cost=1000000015358.19..1000000015358.20 rows=1 width=53)\n",
      "  Merge Key: sum(extended_price)\n",
      "  ->  XN Network  (cost=1000000015358.19..1000000015358.20 rows=1 width=53)\n",
      "        Send to leader\n",
      "        ->  XN Sort  (cost=1000000015358.19..1000000015358.20 rows=1 width=53)\n",
      "              Sort Key: sum(extended_price)\n",
      "              ->  XN HashAggregate  (cost=15358.17..15358.18 rows=1 width=53)\n",
      "                    ->  XN Seq Scan on optimized_orders  (cost=0.00..14143.08 rows=81006 width=53)\n",
      "                          Filter: ((\"ts\" >= '2024-06-01 00:00:00'::timestamp without time zone) AND (\"ts\" < '2024-07-01 00:00:00'::timestamp without time zone) AND (((category)::text = 'compute'::text) OR ((category)::text = 'analytics'::text) OR ((category)::text = 'observability'::text)) AND (((\"region\")::text = 'us-west'::text) OR ((\"region\")::text = 'us-east'::text) OR ((\"region\")::text = 'eu-west'::text)))\n",
      "\n",
      "Query 2 - Spectrum Table EXPLAIN Plan:\n",
      "============================================================\n",
      "XN Merge  (cost=1000275015024.53..1000275015025.28 rows=300 width=206)\n",
      "  Merge Key: sum(derived_col4)\n",
      "  ->  XN Network  (cost=1000275015024.53..1000275015025.28 rows=300 width=206)\n",
      "        Send to leader\n",
      "        ->  XN Sort  (cost=1000275015024.53..1000275015025.28 rows=300 width=206)\n",
      "              Sort Key: sum(derived_col4)\n",
      "              ->  XN HashAggregate  (cost=275015007.69..275015012.19 rows=300 width=206)\n",
      "                    ->  XN S3 Query Scan orders  (cost=275014925.14..275014955.19 rows=3000 width=206)\n",
      "                          ->  S3 HashAggregate  (cost=275014925.14..275014925.19 rows=3000 width=206)\n",
      "                                ->  S3 Seq Scan spectrum_raw.orders location:\"s3://csed516-shared-resources-au2025-sharedclassbucket-mc0zal7gjcmu/class08/raw\" format:ION  (cost=0.00..275000000.00 rows=746257 width=206)\n",
      "                                      Filter: (((category)::text = 'analytics'::text) AND (((\"ts\" < '2023-03-15 00:00:00'::timestamp without time zone) AND (\"ts\" >= '2023-03-01 00:00:00'::timestamp without time zone)) OR ((\"ts\" < '2023-09-15 00:00:00'::timestamp without time zone) AND (\"ts\" >= '2023-09-01 00:00:00'::timestamp without time zone)) OR ((\"ts\" < '2024-03-15 00:00:00'::timestamp without time zone) AND (\"ts\" >= '2024-03-01 00:00:00'::timestamp without time zone))))\n",
      "\n",
      "Query 2 - Native Table EXPLAIN Plan:\n",
      "============================================================\n",
      "XN Merge  (cost=1000000043765.86..1000000043765.87 rows=1 width=43)\n",
      "  Merge Key: sum(extended_price)\n",
      "  ->  XN Network  (cost=1000000043765.86..1000000043765.87 rows=1 width=43)\n",
      "        Send to leader\n",
      "        ->  XN Sort  (cost=1000000043765.86..1000000043765.87 rows=1 width=43)\n",
      "              Sort Key: sum(extended_price)\n",
      "              ->  XN HashAggregate  (cost=43765.84..43765.85 rows=1 width=43)\n",
      "                    ->  XN Seq Scan on optimized_orders  (cost=0.00..41199.52 rows=146647 width=43)\n",
      "                          Filter: ((((\"ts\" >= '2023-09-01 00:00:00'::timestamp without time zone) AND (\"ts\" < '2023-09-15 00:00:00'::timestamp without time zone)) OR ((\"ts\" < '2023-03-15 00:00:00'::timestamp without time zone) AND (\"ts\" >= '2023-03-01 00:00:00'::timestamp without time zone)) OR ((\"ts\" >= '2024-03-01 00:00:00'::timestamp without time zone) AND (\"ts\" < '2024-03-15 00:00:00'::timestamp without time zone))) AND ((category)::text = 'analytics'::text))\n",
      "XN Merge  (cost=1000000015358.19..1000000015358.20 rows=1 width=53)\n",
      "  Merge Key: sum(extended_price)\n",
      "  ->  XN Network  (cost=1000000015358.19..1000000015358.20 rows=1 width=53)\n",
      "        Send to leader\n",
      "        ->  XN Sort  (cost=1000000015358.19..1000000015358.20 rows=1 width=53)\n",
      "              Sort Key: sum(extended_price)\n",
      "              ->  XN HashAggregate  (cost=15358.17..15358.18 rows=1 width=53)\n",
      "                    ->  XN Seq Scan on optimized_orders  (cost=0.00..14143.08 rows=81006 width=53)\n",
      "                          Filter: ((\"ts\" >= '2024-06-01 00:00:00'::timestamp without time zone) AND (\"ts\" < '2024-07-01 00:00:00'::timestamp without time zone) AND (((category)::text = 'compute'::text) OR ((category)::text = 'analytics'::text) OR ((category)::text = 'observability'::text)) AND (((\"region\")::text = 'us-west'::text) OR ((\"region\")::text = 'us-east'::text) OR ((\"region\")::text = 'eu-west'::text)))\n",
      "\n",
      "Query 2 - Spectrum Table EXPLAIN Plan:\n",
      "============================================================\n",
      "XN Merge  (cost=1000275015024.53..1000275015025.28 rows=300 width=206)\n",
      "  Merge Key: sum(derived_col4)\n",
      "  ->  XN Network  (cost=1000275015024.53..1000275015025.28 rows=300 width=206)\n",
      "        Send to leader\n",
      "        ->  XN Sort  (cost=1000275015024.53..1000275015025.28 rows=300 width=206)\n",
      "              Sort Key: sum(derived_col4)\n",
      "              ->  XN HashAggregate  (cost=275015007.69..275015012.19 rows=300 width=206)\n",
      "                    ->  XN S3 Query Scan orders  (cost=275014925.14..275014955.19 rows=3000 width=206)\n",
      "                          ->  S3 HashAggregate  (cost=275014925.14..275014925.19 rows=3000 width=206)\n",
      "                                ->  S3 Seq Scan spectrum_raw.orders location:\"s3://csed516-shared-resources-au2025-sharedclassbucket-mc0zal7gjcmu/class08/raw\" format:ION  (cost=0.00..275000000.00 rows=746257 width=206)\n",
      "                                      Filter: (((category)::text = 'analytics'::text) AND (((\"ts\" < '2023-03-15 00:00:00'::timestamp without time zone) AND (\"ts\" >= '2023-03-01 00:00:00'::timestamp without time zone)) OR ((\"ts\" < '2023-09-15 00:00:00'::timestamp without time zone) AND (\"ts\" >= '2023-09-01 00:00:00'::timestamp without time zone)) OR ((\"ts\" < '2024-03-15 00:00:00'::timestamp without time zone) AND (\"ts\" >= '2024-03-01 00:00:00'::timestamp without time zone))))\n",
      "\n",
      "Query 2 - Native Table EXPLAIN Plan:\n",
      "============================================================\n",
      "XN Merge  (cost=1000000043765.86..1000000043765.87 rows=1 width=43)\n",
      "  Merge Key: sum(extended_price)\n",
      "  ->  XN Network  (cost=1000000043765.86..1000000043765.87 rows=1 width=43)\n",
      "        Send to leader\n",
      "        ->  XN Sort  (cost=1000000043765.86..1000000043765.87 rows=1 width=43)\n",
      "              Sort Key: sum(extended_price)\n",
      "              ->  XN HashAggregate  (cost=43765.84..43765.85 rows=1 width=43)\n",
      "                    ->  XN Seq Scan on optimized_orders  (cost=0.00..41199.52 rows=146647 width=43)\n",
      "                          Filter: ((((\"ts\" >= '2023-09-01 00:00:00'::timestamp without time zone) AND (\"ts\" < '2023-09-15 00:00:00'::timestamp without time zone)) OR ((\"ts\" < '2023-03-15 00:00:00'::timestamp without time zone) AND (\"ts\" >= '2023-03-01 00:00:00'::timestamp without time zone)) OR ((\"ts\" >= '2024-03-01 00:00:00'::timestamp without time zone) AND (\"ts\" < '2024-03-15 00:00:00'::timestamp without time zone))) AND ((category)::text = 'analytics'::text))\n"
     ]
    }
   ],
   "source": [
    "def display_explain_plan(explain_result, query_name, table_type):\n",
    "    print(f\"\\n{query_name} - {table_type} Table EXPLAIN Plan:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if explain_result['Status'] == 'FINISHED':\n",
    "        plan_data = get_query_results(explain_result['Id'])\n",
    "        if plan_data and 'Records' in plan_data:\n",
    "            for record in plan_data['Records']:\n",
    "                if record and len(record) > 0 and 'stringValue' in record[0]:\n",
    "                    print(record[0]['stringValue'])\n",
    "        else:\n",
    "            print(\"No plan data available\")\n",
    "    else:\n",
    "        print(f\"EXPLAIN failed: {explain_result.get('Error', 'Unknown error')}\")\n",
    "\n",
    "display_explain_plan(results['query1']['spectrum']['explain'], \"Query 1\", \"Spectrum\")\n",
    "display_explain_plan(results['query1']['native']['explain'], \"Query 1\", \"Native\")\n",
    "display_explain_plan(results['query2']['spectrum']['explain'], \"Query 2\", \"Spectrum\")\n",
    "display_explain_plan(results['query2']['native']['explain'], \"Query 2\", \"Native\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ad69c",
   "metadata": {},
   "source": [
    "### 2. Storage Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a12cba80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storage Comparison:\n",
      "==============================================================================================================\n",
      "Storage Location   Format    Size                                                                     Notes\n",
      "     S3 Spectrum     JSON   ~3 GB        S3 storage is larger due to JSON text format and metadata overhead\n",
      " Redshift Native Columnar 2332 MB Redshift native storage is comparatively very small using columnar format\n"
     ]
    }
   ],
   "source": [
    "# Storage Comparison Table\n",
    "print(\"Storage Comparison:\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "storage_data = [\n",
    "    [\"S3 Spectrum\", \"JSON\", \"~3 GB\", \"S3 storage is larger due to JSON text format and metadata overhead\"],\n",
    "    [\"Redshift Native\", \"Columnar\", \"2332 MB\", \"Redshift native storage is comparatively very small using columnar format\"]\n",
    "]\n",
    "\n",
    "storage_df = pd.DataFrame(storage_data, columns=[\"Storage Location\", \"Format\", \"Size\", \"Notes\"])\n",
    "print(storage_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a886f891",
   "metadata": {},
   "source": [
    "### 3. Execution Time Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb8f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Performance Comparison:\n",
      "                          Query Spectrum Time  Native Time Speedup\n",
      "Query 1 (Selective Time Filter)         65.44s       5.26s   12.5x\n",
      " Query 2 (Multi-range Temporal)         63.98s       6.50s    9.8x\n"
     ]
    }
   ],
   "source": [
    "# Create performance comparison table\n",
    "performance_data = []\n",
    "\n",
    "q1_spectrum_time = results['query1']['spectrum']['execution'].get('Duration', 0) / 1_000_000_000\n",
    "q1_native_time = results['query1']['native']['execution'].get('Duration', 0) / 1_000_000_000\n",
    "q2_spectrum_time = results['query2']['spectrum']['execution'].get('Duration', 0) / 1_000_000_000\n",
    "q2_native_time = results['query2']['native']['execution'].get('Duration', 0) / 1_000_000_000\n",
    "\n",
    "q1_speedup = q1_spectrum_time / q1_native_time if q1_native_time > 0 else 0\n",
    "q2_speedup = q2_spectrum_time / q2_native_time if q2_native_time > 0 else 0\n",
    "\n",
    "performance_data = [\n",
    "    ['Query 1 (Selective Time Filter)', f\"{q1_spectrum_time:.2f}s\", f\"{q1_native_time:.2f}s\", f\"{q1_speedup:.1f}x\"],\n",
    "    ['Query 2 (Multi-range Temporal)', f\"{q2_spectrum_time:.2f}s\", f\"{q2_native_time:.2f}s\", f\"{q2_speedup:.1f}x\"]\n",
    "]\n",
    "\n",
    "performance_df = pd.DataFrame(performance_data, \n",
    "                            columns=['Query', 'Spectrum Time ', 'Native Time', 'Speedup'])\n",
    "\n",
    "print(\"Query Performance Comparison:\")\n",
    "\n",
    "print(performance_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26daeb18",
   "metadata": {},
   "source": [
    "## Part 5 - Export Aggregated Data with UNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c58ef24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Storage Comparison:\n",
      "==============================================================================================================\n",
      "    Storage Location   Format     Size  Row Count                                     Notes\n",
      "         S3 Spectrum     JSON    ~3 GB 30,000,000          Original source data, text-based\n",
      "     Redshift Native Columnar  2332 MB 30,000,000           From svv_table_info size column\n",
      "S3 Summary (Parquet)  Parquet 168.4 KB     ~4,380 Aggregated output, columnar + compression\n"
     ]
    }
   ],
   "source": [
    "# Document Storage Comparison\n",
    "print(\"Document Storage Comparison:\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "storage_data = [\n",
    "    [\"S3 Spectrum\", \"JSON\", \"~3 GB\", \"30,000,000\" ,\"Original source data, text-based\"],\n",
    "    [\"Redshift Native\", \"Columnar\", \"2332 MB\", \"30,000,000\" , \"From svv_table_info size column\"],\n",
    "    [\"S3 Summary (Parquet)\", \"Parquet\", \"168.4 KB\",\"~4,380\", \"Aggregated output, columnar + compression\"]\n",
    "]\n",
    "\n",
    "storage_df = pd.DataFrame(storage_data, columns=[\"Storage Location\", \"Format\", \"Size\", \"Row Count\", \"Notes\"])\n",
    "print(storage_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c5600",
   "metadata": {},
   "source": [
    "### Analysis Questions:\n",
    "\n",
    "**1. How does the summary Parquet size compare to the raw JSON source? What explains the dramatic reduction?**\n",
    "\n",
    "There is a drastic reduction in the size. The original JSON storage weighed 2-3 GB whereas the Parquet was just in KBs. \n",
    "\n",
    "\n",
    "**2. What is the compression ratio from 30M rows to ~4,380 rows? What aggregation caused this?**\n",
    "\n",
    "The compression ratio was 6849x. The aggregation of COUNT, SUM, and AVG around revenue_flag classification logic and GROUP BY caused this.\n",
    "\n",
    "\n",
    "**3. How many partition directories were created? What does each represent?**\n",
    "\n",
    "Three partition directories were created based on the revenue_flag classification:\n",
    "\n",
    "1. revenue_flag=high\n",
    "2. revenue_flag=normal\n",
    "3. revenue_flag=low\n",
    "\n",
    "Each partition represents a business-defined revenue performance tier that enables efficient anomaly detection and monitoring.\n",
    "\n",
    "\n",
    "**4. How does partitioning by revenue_flag enable efficient downstream queries?**\n",
    "\n",
    "Partitioning by revenue_flag enables partition pruning, where queries that filter by revenue performance only scan relevant directories. For example, a query looking for high-revenue anomalies (WHERE revenue_flag = 'high') would only scan the high/ directory, skipping approximately 66% of the data in normal and low partitions.\n",
    "\n",
    "\n",
    "**5. What business value does this pre-aggregated dataset provide for dashboards and monitoring?**\n",
    "\n",
    "This dataset provides significant business value through faster executive dashboards that can efficiently filter by performance tiers. It enables historical trend analysis for forecasting, category performance comparison across time periods, and consistent business logic through revenue_flag definitions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c01c101",
   "metadata": {},
   "source": [
    "### UNLOAD Operation Documentation\n",
    "\n",
    "\n",
    "- **Execution Duration**: [Insert duration from describe-statement response] seconds\n",
    "\n",
    "\n",
    "- **Number of Parquet Files Created**: 12\n",
    "- **Total Output Size**: 168.4 KiB\n",
    "\n",
    "- **Partition Structure**\n",
    "    - **revenue_flag=high**: Days exceeding $25,000,000 in daily revenue\n",
    "    - **revenue_flag=normal**: Days with revenue between $700,000 and $25,000,000\n",
    "    - **revenue_flag=low**: Days with revenue below $700,000\n",
    "\n",
    "- **Row Count**: 4,380 rows\n",
    "\n",
    "- **Storage Efficiency Analysis:**\n",
    "\n",
    "| Storage Method | Format | Size | Row Count | Notes |\n",
    "|----------------|--------|------|-----------|-------|\n",
    "| S3 Spectrum (Source) | JSON | ~3 GB | 30,000,000 | Original raw transaction data |\n",
    "| Redshift Native | Columnar | 2,332 MB | 30,000,000 | Compressed columnar storage |\n",
    "| S3 Summary (UNLOAD) | Parquet | 168.4 KiB | 4,380 | Daily aggregated summaries |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeacd18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1de1a336",
   "metadata": {},
   "source": [
    "## Reflection:\n",
    "\n",
    "\n",
    "\n",
    "In this lab, I implemented a complete data workflow using Amazon Redshift that demonstrated modern data lake and warehouse integration patterns. The workflow began with Redshift Spectrum, which allowed me to query 30 million JSON records stored directly in S3 without loading them into Redshift. This external table approach provided immediate access to the raw data but came with performance limitations due to the text-based JSON format and network overhead of querying data across S3.\n",
    "The next phase involved loading this data into a native Redshift table with optimized storage characteristics. By using columnar compression, sort keys, and distribution styles, I transformed the same 30 million rows from approximately 3GB of S3 storage to just 2332 MB of highly compressed native storage. The performance improvements were dramatic, with native queries executing significantly faster than their counterparts leading to 10x speedup on an approximate average.\n",
    "\n",
    "The final component involved using UNLOAD to export aggregated daily summaries back to S3 as partitioned Parquet files. This process reduced 30 million individual transactions to approximately 4,380 daily summary records, achieving a compression ratio of 6849:1. The resulting Parquet files partitioned by revenue performance flags, created an optimized dataset for downstream analytics tools.\n",
    "\n",
    "\n",
    "Architectural Trade-offs:\n",
    "\n",
    "Spectrum excels when we need to query large amounts of infrequently accessed data (aka cold data) without the overhead of loading and maintaining it in Redshift. This approach works well for exploratory data analysis, archival queries, or when storage costs outweigh query performance requirements. However, Spectrum queries are inherently slower due to network latency and JSON parsing overhead.\n",
    "\n",
    "Native Redshift tables should be chosen when query performance is critical or when data is accessed frequently (aka hot). The upfront cost of loading and storing data in Redshift pays back through faster query execution, less storage, and other advanced features.\n",
    "\n",
    "\n",
    "UNLOAD and Data Interoperability:\n",
    "\n",
    "The UNLOAD operation represents a crucial component of modern data architectures by enabling the S3 to Redshift to S3 roundtrip pattern. This workflow allows organizations to leverage Redshift's analytical capabilities while maintaining data in the more flexible and cost-effective S3 ecosystem. Common use cases include pre-computing expensive aggregations for dashboard consumption, creating optimized datasets, and sharing processed data with external systems.\n",
    "\n",
    "Parquet format provides exceptional cross-tool interoperability, enabling the same dataset to be consumed by Athena or Spark. The columnar structure and built-in compression make Parquet faster for analytical workloads across different computing environments. The partitioning strategy we implemented using revenue flags enables efficient partition pruning, allowing downstream tools to scan only relevant data subsets.\n",
    "\n",
    "\n",
    "Comparison to Previous Tools:\n",
    "\n",
    "Compared to Athena from Week 3, Redshift Spectrum provides similar serverless S3 querying capabilities but with the added benefit of being integrated into a full data warehouse ecosystem. If I remember correctly from class, it was also mentioned that Redshift is better for IAM.\n",
    "\n",
    "The decision between provisioned Redshift and serverless Athena depends primarily on usage patterns and performance requirements. Redshift becomes cost-effective when query volume is high and predictable, when sub-second response times are required, or when complex workload management is needed. Athena suits sporadic querying, cost-sensitive environments, and scenarios where infrastructure management overhead should be minimized. Polars from Week 2 serve as complementary tools in this ecosystem.\n",
    "\n",
    "\n",
    "Production Considerations:\n",
    "\n",
    "The decision to load data into Redshift versus keeping it in S3 are usually driven by access patterns, performance requirements, and cost considerations. Data that requires frequent access, complex joins, or superfast response times benefits from native Redshift storage. Archival/Cold data, infrequently accessed historical records, or datasets primarily used for batch processing are better suited for S3 storage with occasional Spectrum access.\n",
    "Exporting optimized data back to S3 becomes valuable when creating curated datasets for specific use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f1420",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
